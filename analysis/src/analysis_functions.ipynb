{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, recall_score\n",
    "from adjustText import adjust_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_associations(output_dir, min_n=None):\n",
    "    plt.rcdefaults()\n",
    "\n",
    "    all_associations = pd.DataFrame()\n",
    "    files = [i for i in os.listdir(output_dir + '/data') if 'ratios_all.txt' in i]\n",
    "\n",
    "    # Loop over all targets\n",
    "    for file in files:  \n",
    "        target_df = pd.read_csv(output_dir + '/data/{}'.format(file), sep='\\t')\n",
    "\n",
    "        if len(target_df) < 1:\n",
    "            continue\n",
    "\n",
    "        all_associations = all_associations.append(target_df)\n",
    "    \n",
    "    if min_n:\n",
    "        all_associations = all_associations.loc[(all_associations['nr compounds with AE']>=min_n)&(all_associations['nr compounds active']>=min_n)&(~all_associations['Likelihood Ratio'].isnull()),:]\n",
    "    \n",
    "    return all_associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_associations(output_dir, min_n, lr, pv, target_info):\n",
    "    plt.rcdefaults()\n",
    "\n",
    "    all_associations = pd.DataFrame()\n",
    "    files = [i for i in os.listdir(output_dir + '/data') if 'ratios.txt' in i]\n",
    "\n",
    "    # Loop over all targets\n",
    "    for file in files:  \n",
    "        target_df = pd.read_csv(output_dir + '/data/{}'.format(file), sep='\\t')\n",
    "        target_df = target_df.loc[(target_df['nr compounds with AE']>=min_n)&(target_df['nr compounds active']>=min_n)&(~target_df['Likelihood Ratio'].isnull()),:]\n",
    "\n",
    "        if len(target_df) < 1:\n",
    "            continue\n",
    "\n",
    "        all_associations = all_associations.append(target_df)\n",
    "    \n",
    "    merged = all_associations.merge(target_info, on='accession', how='left')\n",
    "    significant = merged.loc[(merged['corrected p-value']<= pv)&(merged['Likelihood Ratio']>= lr),:]\n",
    "    \n",
    "    return merged, significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_promiscuity_plots(assoc_df, output_dir):\n",
    "    plt.rcdefaults()\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "    target_compounds = assoc_df[['pref_name','molregnos','activity_vector']].drop_duplicates()\n",
    "    target2compounds = dict()\n",
    "\n",
    "    for item in target_compounds.iterrows():\n",
    "        target_name = item[1][0]\n",
    "        molregnos = [int(float(i)) for i in item[1][1].strip('[]').split(', ')]\n",
    "        activities = [int(float(i)) for i in item[1][2].strip('[]').split(', ')]\n",
    "        active_compounds = [molregno for molregno, activity in zip(molregnos, activities) if activity==1]\n",
    "        target_name_ext = target_name + ' ({})'.format(len(set(active_compounds)))\n",
    "        target2compounds[target_name_ext] = set(active_compounds)\n",
    "    \n",
    "    sorted_targets = sorted(target2compounds.keys())\n",
    "    overlap_data = dict()\n",
    "\n",
    "    for target_1 in sorted_targets:\n",
    "        overlap_vector = []\n",
    "        for target_2 in sorted_targets:\n",
    "            overlap_perc = len(target2compounds[target_1] & target2compounds[target_2])/len(target2compounds[target_1])\n",
    "            overlap_vector.append(overlap_perc)\n",
    "        overlap_data[target_1] = overlap_vector\n",
    "    overlap_matrix_target1 = pd.DataFrame.from_dict(overlap_data, orient='index', columns=sorted_targets)\n",
    "\n",
    "    plt.rc('axes', labelsize=20)\n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    \n",
    "    current_date = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "    fig = plt.figure(figsize=(16,16))\n",
    "    g = sns.heatmap(overlap_matrix_target1, square=True,cmap='afmhot_r',xticklabels=True,yticklabels=True,  cbar_kws={'shrink':0.3, 'label':'Fraction of active compounds overlapping'})#annot=True,annot_kws={'size':9},\n",
    "    g.set_ylabel('Proteins for which fraction is displayed (Nr. active compounds)', size=20) # overlapping fraction is shown\n",
    "    g.set_xlabel('Proteins (Nr. active compounds)', size=20)\n",
    "    fig.savefig(output_dir + '/{}_promiscuity.png'.format(current_date), dpi=200, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ppv(row):\n",
    "    ae_active = int(row['nr compounds with AE'] * row['ae_hit_rate'])\n",
    "    ppv = ae_active / row['nr compounds active']\n",
    "    return ppv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_hit_rate_ppv_plot(df1, df1_name, df1_color, df2, df2_name, df2_color, output_dir, annotations=None, target_abbreviations=None, additional_filename='', alpha=0.6):\n",
    "    plt.rcdefaults()\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "    markers = iter(['x','x'])\n",
    "    colours = iter([df2_color, df1_color])\n",
    "    distances = [0.1,0.2,0.3,0.4]\n",
    "\n",
    "    for df in [df1, df2]:\n",
    "        df['PPV'] = df.apply(lambda x: calculate_ppv(x), axis=1)\n",
    "    \n",
    "    df1['kind'] = df1_name\n",
    "    df2['kind'] = df2_name\n",
    "    concat_df = pd.concat([df1, df2], sort=False)\n",
    "    concat_df.rename(columns={'ae_hit_rate': \"Fraction of AE-associated drugs that are active\"}, inplace=True)\n",
    "    \n",
    "    def multivariateGrid(output_dir, col_x, col_y, col_k, df, k_is_color=False, scatter_alpha=alpha):\n",
    "        plt.rc('axes', labelsize=14)\n",
    "        plt.rcParams['xtick.labelsize'] = 14\n",
    "        plt.rcParams['ytick.labelsize'] = 14\n",
    "\n",
    "        def colored_scatter(x,y,c=None):\n",
    "            def scatter(*args, **kwargs):\n",
    "                args = (x, y)\n",
    "                if c is not None:\n",
    "                    kwargs['c'] = c\n",
    "                kwargs['alpha'] = scatter_alpha\n",
    "                kwargs['s'] = 7\n",
    "                kwargs['marker'] = next(markers)\n",
    "                plt.scatter(*args, **kwargs)\n",
    "            \n",
    "            return scatter\n",
    "\n",
    "        g = sns.JointGrid(\n",
    "            x=col_x,\n",
    "            y=col_y,\n",
    "            data=df\n",
    "        )\n",
    "\n",
    "        legends=[]\n",
    "        for name, df_group in df.groupby(col_k):\n",
    "            color = next(colours)\n",
    "            legends.append(name)\n",
    "            if k_is_color:\n",
    "                color=name\n",
    "            g.plot_joint(\n",
    "                colored_scatter(df_group[col_x],df_group[col_y],color),\n",
    "            )\n",
    "            \n",
    "            sns.distplot(\n",
    "                df_group[col_x].values,\n",
    "                ax=g.ax_marg_x,\n",
    "                color=color,\n",
    "                kde=False,\n",
    "                norm_hist=True,\n",
    "                bins=np.arange(0, 1.05, 0.05)\n",
    "            )\n",
    "            sns.distplot(\n",
    "                df_group[col_y].values,\n",
    "                ax=g.ax_marg_y,\n",
    "                color=color,\n",
    "                vertical=True,\n",
    "                kde=False,\n",
    "                norm_hist=True,\n",
    "                bins=np.arange(0, 1.05, 0.05)\n",
    "            )\n",
    "        plt.legend(legends, loc=1, fontsize=12) # bbox_to_anchor=(1.2, 1)\n",
    "        \n",
    "        if annotations:\n",
    "            for annotation in annotations:\n",
    "                plt.annotate(annotation[0], xy=annotation[1], xytext=annotation[2], arrowprops=dict(arrowstyle=\"->\",connectionstyle=\"arc3\", alpha=0.7, color='darkgrey'), fontsize=8, verticalalignment='bottom')\n",
    "\n",
    "        if target_abbreviations:\n",
    "            plt.text(x=0, y=-0.2, s=target_abbreviations, fontsize=10, alpha=0.8)\n",
    "        \n",
    "        plt.xlim([0, 1.1])\n",
    "        current_date = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "        plt.savefig(output_dir + '/{}_hit_rates_va-PPV{}.png'.format(current_date, additional_filename), dpi=200, bbox_inches='tight')\n",
    "        \n",
    "        \n",
    "    multivariateGrid(output_dir, col_y='Value-added PPV', col_x='Fraction of AE-associated drugs that are active', df = concat_df, col_k='kind')\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_heatmap_dict(assoc_df, meddra_hier, colour_dict_loc):\n",
    "    plt.rcdefaults()\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "    # Make an SOC colour dict that can stay consistent between plots\n",
    "\n",
    "    colour_dict = dict()\n",
    "    common_socs = assoc_df.merge(meddra_hier, left_on='Adverse Event', right_on=' Term').groupby('SOC').count().sort_values(by='Adverse Event', ascending=False).index\n",
    "    colour_list = ['#f46d43','#fdae61','#fee090','#ffffbf','#e0f3f8','#abd9e9','#74add1','#4575b4','#313695'] + [\"#9970ab\", \"#c2a5cf\", \"#e7d4e8\"] + [\"#c7eae5\", \"#80cdc1\", \"#35978f\", \"#01665e\"] + [\"#8c510a\",\"#bf812d\", \"#dfc27d\", \"#f6e8c3\", \"#543005\", \"#f6e8c3\"]\n",
    "\n",
    "    for soc, colour in zip(common_socs, colour_list[:len(common_socs)]):\n",
    "        colour_dict[soc] = colour\n",
    "\n",
    "    # Add rest of SOCS\n",
    "    rest_socs = set(meddra_hier['SOC'].drop_duplicates()) - set(colour_dict.keys())\n",
    "    for soc, colour in zip(rest_socs, sns.color_palette(\"dark\",len(rest_socs))):\n",
    "        colour_dict[soc] = colour\n",
    "    \n",
    "    with open(colour_dict_loc, 'wb') as f:\n",
    "        pickle.dump(colour_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_heatmap(assoc_df, clustering_method, meddra_hier, colour_dict_loc, output_loc, output_filename_conditions):\n",
    "    plt.rcdefaults()\n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    plt.rc('axes', labelsize=18)\n",
    "    #sns.set(font_scale=1.5)\n",
    "    \n",
    "    with open(colour_dict_loc, 'rb') as f:\n",
    "        imported_colour_dict = pickle.load(f)\n",
    "    \n",
    "    pt2colour = dict()\n",
    "    for item in meddra_hier[[' Term', 'SOC']].drop_duplicates().iterrows():\n",
    "        pt2colour[item[1][0]] = imported_colour_dict[item[1][1]]\n",
    "    \n",
    "    def find_colour(x):\n",
    "        try:\n",
    "            colour = pt2colour[x]\n",
    "            return colour\n",
    "        except KeyError:\n",
    "            return 'w'\n",
    "\n",
    "    unique_aes = sorted(list(assoc_df['Adverse Event'].drop_duplicates()))\n",
    "    all_targets = list(assoc_df['pref_name'].drop_duplicates())\n",
    "\n",
    "    # Make matrix\n",
    "    my_dict = dict()\n",
    "\n",
    "    for target in all_targets:\n",
    "        target_df = assoc_df.loc[assoc_df['pref_name']==target,['Likelihood Ratio','Adverse Event']]\n",
    "        target_aes = list(target_df['Adverse Event'].drop_duplicates())\n",
    "        lr_vector = []\n",
    "        for ae in unique_aes:\n",
    "            if ae in target_aes:\n",
    "                my_lr = list(target_df.loc[target_df['Adverse Event']==ae,'Likelihood Ratio'])[0]\n",
    "                if my_lr == np.inf:\n",
    "                    my_lr = max(assoc_df.loc[assoc_df['Likelihood Ratio']!=np.inf,'Likelihood Ratio'])\n",
    "            else:\n",
    "                my_lr = 1\n",
    "            lr_vector.append(my_lr)\n",
    "        my_dict[target] = lr_vector   \n",
    "    ae_matrix = pd.DataFrame.from_dict(my_dict, orient='index', columns=[i.upper() for i in unique_aes])\n",
    "\n",
    "    # Do plot\n",
    "    my_colours = pd.Series(ae_matrix.columns.map(find_colour))\n",
    "    my_colours.name=('MedDRA System Organ Class')\n",
    "    my_colours.index=ae_matrix.columns\n",
    "\n",
    "    plt.figure(figsize=(150, 150))\n",
    "\n",
    "    g = sns.clustermap(ae_matrix, method=clustering_method, yticklabels=1, metric='euclidean',cmap='gist_heat_r',xticklabels=0, col_colors=my_colours, cbar_kws={'label':'Likelihood Ratio (LR)'}) # 'ticks':ticks\n",
    "\n",
    "    #g.ax_row_dendrogram.set_visible(False)\n",
    "    #g.ax_col_dendrogram.set_visible(False)\n",
    "\n",
    "    g.cax.set_position([0, .2, .03, .45])\n",
    "\n",
    "    #g.cax.set_yticklabels(ticklabels)\n",
    "\n",
    "    plt.text(0,min(g.cax.get_yticks()-g.cax.get_yticks()[1]),'Not strongly\\nassociated\\nor no data', wrap=True, size=14)\n",
    "    plt.text(0,(max(g.cax.get_yticks())+g.cax.get_yticks()[1])/1.2,'Strongly\\nassociated', wrap=True, size=14)\n",
    "    \n",
    "    ax = g.ax_heatmap\n",
    "    ax.set_xlabel('{} adverse events'.format(len(unique_aes)), size=18)\n",
    "    ax.set_ylabel('{} proteins'.format(len(all_targets)), size=18)\n",
    "    ax.set_yticklabels(g.ax_heatmap.get_ymajorticklabels(), fontsize = 12)\n",
    "    \n",
    "    current_date = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "    plt.savefig(output_loc + '/{}_ae_heatmap_{}.png'.format(current_date, output_filename_conditions), dpi=200, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pm_heatmap(assoc_df, meddra_hier, pm_enriched, colour_dict_loc, output_loc, output_filename_conditions, clustering_method='complete'):\n",
    "    plt.rcdefaults()\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    \n",
    "    # Import colour dict\n",
    "    with open(colour_dict_loc, 'rb') as f:\n",
    "        imported_colour_dict = pickle.load(f)\n",
    "    \n",
    "    pt2colour = dict()\n",
    "    for item in meddra_hier[[' Term', 'SOC']].drop_duplicates().iterrows():\n",
    "        pt2colour[item[1][0]] = imported_colour_dict[item[1][1]]\n",
    "    \n",
    "    def find_colour(x):\n",
    "        try:\n",
    "            colour = pt2colour[x]\n",
    "            return colour\n",
    "        except KeyError:\n",
    "            return 'w'\n",
    "\n",
    "    # Make matrix\n",
    "    pm_assoc = assoc_df.loc[assoc_df['Adverse Event'].isin([i.upper() for i in pm_enriched.index]),:]\n",
    "    pm_aes = sorted(list(pm_assoc['Adverse Event'].drop_duplicates()))\n",
    "    if len(pm_aes) < 2:\n",
    "        return\n",
    "    \n",
    "    pm_targets = list(pm_assoc['pref_name'].drop_duplicates())    \n",
    "    \n",
    "    my_dict = dict()\n",
    "\n",
    "    for target in pm_targets:\n",
    "        target_df = pm_assoc.loc[pm_assoc['pref_name']==target,['Likelihood Ratio','Adverse Event']]\n",
    "        target_aes = list(target_df['Adverse Event'].drop_duplicates())\n",
    "\n",
    "        lr_vector = []\n",
    "\n",
    "        for ae in pm_aes:\n",
    "            if ae in target_aes:\n",
    "                my_lr = list(target_df.loc[target_df['Adverse Event']==ae,'Likelihood Ratio'])[0]\n",
    "                if my_lr == np.inf:\n",
    "                    my_lr = max(pm_assoc.loc[pm_assoc['Likelihood Ratio']!=np.inf,'Likelihood Ratio'])\n",
    "            else:\n",
    "                my_lr = 1\n",
    "\n",
    "            lr_vector.append(my_lr)\n",
    "\n",
    "        my_dict[target] = lr_vector   \n",
    "    ae_matrix = pd.DataFrame.from_dict(my_dict, orient='index', columns=[i.lower().capitalize() for i in pm_aes])\n",
    "    \n",
    "    if len(my_dict) < 2:\n",
    "        return\n",
    "    \n",
    "    # Do plot\n",
    "    my_colours = pd.Series([i.upper() for i in ae_matrix.columns]).map(find_colour)\n",
    "    my_colours.name=('MedDRA System Organ Class')\n",
    "    my_colours.index=ae_matrix.columns\n",
    "\n",
    "    plt.figure(figsize=(150, 150))\n",
    "    sns.set(font_scale=1.5)\n",
    "    g = sns.clustermap(ae_matrix, method=clustering_method, col_colors=my_colours, yticklabels=1, cmap='gist_yarg', metric='euclidean',xticklabels=True, cbar_kws={'label':'Likelihood Ratio (LR)'})#'ticks':[1, 4,8,12,16,20]\n",
    "\n",
    "    g.cax.set_position([-.10, .2, .03, .45])\n",
    "\n",
    "    plt.text(0.1,min(g.cax.get_yticks()-g.cax.get_yticks()[1]),'Not strongly\\nassociated\\nor no data', wrap=True)\n",
    "    plt.text(0.1,(max(g.cax.get_yticks())+g.cax.get_yticks()[1])/1.2,'Strongly\\nassociated', wrap=True)\n",
    "\n",
    "    current_socs = list(pm_assoc.merge(meddra_hier, left_on='Adverse Event', right_on=' Term', suffixes=['x','']).groupby('SOC').count().sort_values(by='Adverse Event', ascending=False).index)\n",
    "    #current_socs.append('Not classified')\n",
    "\n",
    "    for label in current_socs:\n",
    "         g.ax_col_dendrogram.bar(0, 0, color=imported_colour_dict[label],label=label, linewidth=0)\n",
    "    g.ax_col_dendrogram.legend(loc='right', ncol=2, bbox_to_anchor=(1.3, 1.8), title='MedDRA System Organ Class',framealpha=0)\n",
    "\n",
    "    ax = g.ax_heatmap\n",
    "    ax.set_xlabel('Post-marketing relevant adverse events', size=20)\n",
    "    ax.set_ylabel('Proteins', size=20)\n",
    "    plt.rc('axes', labelsize=12)\n",
    "\n",
    "    current_date = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "    plt.savefig(output_loc + '/{}_ae_heatmap_pm{}.png'.format(current_date, output_filename_conditions), dpi=200, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_soc_donut(assoc_df, meddra_hier, colour_dict_loc, output_loc, output_filename_conditions, sizes=[1,1,1,1], white_txt_nrs=[]):\n",
    "    plt.rcdefaults()\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    \n",
    "    with open(colour_dict_loc, 'rb') as f:\n",
    "        imported_colour_dict = pickle.load(f)\n",
    "\n",
    "    plt.rcParams['font.size'] = 12\n",
    "    my_data = assoc_df.merge(meddra_hier, how='left',left_on='Adverse Event', right_on=' Term')\n",
    "    \n",
    "    data_values = list(my_data.groupby('SOC').count().sort_values(by='Adverse Event', ascending=False)['accession'].values)\n",
    "    data_index = list(my_data.groupby('SOC').count().sort_values(by='Adverse Event', ascending=False)['accession'].index)\n",
    "\n",
    "    data_index_large_only = [i if j > sizes[0]  else '' for i,j in zip(data_index, data_values)]\n",
    "    data_index_count = [True if j < sizes[1] else False for j in data_values]\n",
    "    data_index_medium = [i if j > sizes[1]  else '' for i,j in zip(data_index, data_values)]\n",
    "\n",
    "    def make_autopct(values):\n",
    "        def my_autopct(pct):\n",
    "            total = sum(values)\n",
    "            val = int(round(pct*total/100.0))\n",
    "            return '{v:d}'.format(v=val)\n",
    "        return my_autopct\n",
    "    def absolute_value(val):\n",
    "        a  = int(np.round(val/100.*sum(data_values), 0))\n",
    "        if a > sizes[2]:\n",
    "            return a\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    wedges, texts, autotexts = ax.pie(data_values, wedgeprops=dict(width=0.3,edgecolor='white'), pctdistance=0.85, startangle=0, labels=data_index_large_only, colors=[imported_colour_dict[x] for x in data_index], autopct=absolute_value)\n",
    "    for nr, autotext in enumerate(autotexts):\n",
    "        if nr in white_txt_nrs:\n",
    "            autotext.set_color('white')\n",
    "\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0\", fc=\"w\", ec=\"k\", lw=0.72)\n",
    "    kw = dict(arrowprops=dict(arrowstyle=\"-\"),zorder=0, va=\"center\") # bbox=bbox_props, \n",
    "\n",
    "    for i, p in enumerate(wedges[sizes[3]:len([i for i in data_index_medium if i!=''])], start=sizes[3]):\n",
    "        ang = (p.theta2 - p.theta1)/2. + p.theta1\n",
    "        y = np.sin(np.deg2rad(ang))\n",
    "        x = np.cos(np.deg2rad(ang))\n",
    "        horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n",
    "        connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n",
    "        kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle, \"color\": 'black'})\n",
    "        ax.annotate(data_index_medium[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.6*y),\n",
    "                    horizontalalignment=horizontalalignment, **kw)\n",
    "\n",
    "    plt.text(-0.5,0,'      {}\\nprotein-AE pairs'.format(len(assoc_df)))\n",
    "\n",
    "    current_date = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "    plt.savefig(output_loc + '/{}_ae_soc_donut_{}.png'.format(current_date, output_filename_conditions), dpi=200, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_atc_bar_plot(assoc_df, all_atc_codes_loc, small_molecule_atc_codes_loc, output_loc):\n",
    "    plt.rcdefaults()\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    plt.figure(figsize=(8, 6)) \n",
    "    sns.set_style(\"white\")\n",
    "    plt.rc('axes', labelsize=15)\n",
    "    \n",
    "    all_molregnos = set([int(x) for i in list(assoc_df['molregnos'].drop_duplicates()) for x in i.strip('[]').split(', ')])\n",
    "\n",
    "    all_atc = pd.read_csv(all_atc_codes_loc, sep='\\t')\n",
    "    project_atc = all_atc.loc[all_atc['molregno'].isin(all_molregnos),:]\n",
    "    background_atc = pd.read_csv(small_molecule_atc_codes_loc, sep='\\t')\n",
    "\n",
    "    project_len = len(project_atc)\n",
    "    background_len = len(background_atc)\n",
    "\n",
    "    background_atc_counts = background_atc.groupby('level1_description').count().sort_values(by='molregno',ascending=False)[['molregno']]\n",
    "    background_atc_counts.index = [i.lower().capitalize() for i in background_atc_counts.index]\n",
    "    background_atc_counts.reset_index(inplace=True)\n",
    "\n",
    "    project_atc_counts = project_atc.groupby('level1_description').count().sort_values(by='molregno',ascending=False)[['molregno']]\n",
    "    project_atc_counts.index = [i.lower().capitalize() for i in project_atc_counts.index]\n",
    "    project_atc_counts.reset_index(inplace=True)\n",
    "\n",
    "    comparison = background_atc_counts.merge(project_atc_counts, how='outer', on='index')\n",
    "    comparison.columns = ['ATC level 1 description', 'Small molecule approved drugs count', 'Drugs in dataset analysed count']\n",
    "    comparison.set_index('ATC level 1 description', drop=True,inplace=True)\n",
    "\n",
    "    comparison['Small molecule approved drugs %'] = comparison['Small molecule approved drugs count'].apply(lambda x: x/sum(comparison['Small molecule approved drugs count'])*100)\n",
    "    comparison['Dataset drugs %'] = comparison['Drugs in dataset analysed count'].apply(lambda x: x/sum(comparison['Drugs in dataset analysed count'])*100)\n",
    "\n",
    "    comparison_r = comparison.sort_values(by='Small molecule approved drugs %')\n",
    "    ind = np.arange(len(comparison_r))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    width = 0.35\n",
    "\n",
    "    ax.barh(ind, comparison_r['Small molecule approved drugs %'], width, label='Approved small molecule drugs (ChEMBL) ({} ATC codes)'.format(background_len), color='darkgrey')\n",
    "    ax.barh(ind + width, comparison_r['Dataset drugs %'], width, label='Dataset drugs ({} ATC codes)'.format(project_len), color='#67a9cf')\n",
    "\n",
    "    ax.set(yticks=ind + width, yticklabels=comparison_r.index, ylim=[2*width - 1, len(comparison_r)])\n",
    "    ax.set_xlabel('Percentage of ATC labels')\n",
    "\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    order = [1,0]\n",
    "    plt.legend([handles[idx] for idx in order],[labels[idx] for idx in order],loc=(0,1.05))\n",
    "\n",
    "    #ax.set_xticks([i for i in range(1,21,2)])\n",
    "\n",
    "    current_date = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "    plt.savefig(output_loc + '/{}_ATC_bar.png'.format(current_date) , dpi=200, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_atc_bar_plot_three_datasets(assoc_df1, df1_name, df1_color, assoc_df2, df2_name, df2_color, all_atc_codes_loc, small_molecule_atc_codes_loc, output_loc):\n",
    "    plt.rcdefaults()\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    plt.figure(figsize=(8, 6)) \n",
    "    sns.set_style(\"white\")\n",
    "    plt.rc('axes', labelsize=15)\n",
    "    \n",
    "    df1_molregnos = set([int(x) for i in list(assoc_df1['molregnos'].drop_duplicates()) for x in i.strip('[]').split(', ')])\n",
    "    df2_molregnos = set([int(x) for i in list(assoc_df2['molregnos'].drop_duplicates()) for x in i.strip('[]').split(', ')])\n",
    "\n",
    "    all_atc = pd.read_csv(all_atc_codes_loc, sep='\\t')\n",
    "    df1_atc = all_atc.loc[all_atc['molregno'].isin(df1_molregnos),:]\n",
    "    df2_atc = all_atc.loc[all_atc['molregno'].isin(df2_molregnos),:]\n",
    "    background_atc = pd.read_csv(small_molecule_atc_codes_loc, sep='\\t')\n",
    "\n",
    "    df1_len = len(df1_atc)\n",
    "    df2_len = len(df2_atc)\n",
    "    background_len = len(background_atc)\n",
    "\n",
    "    background_atc_counts = background_atc.groupby('level1_description').count().sort_values(by='molregno',ascending=False)[['molregno']]\n",
    "    background_atc_counts.index = [i.lower().capitalize() for i in background_atc_counts.index]\n",
    "    background_atc_counts.reset_index(inplace=True)\n",
    "\n",
    "    df1_atc_counts = df1_atc.groupby('level1_description').count().sort_values(by='molregno',ascending=False)[['molregno']]\n",
    "    df1_atc_counts.index = [i.lower().capitalize() for i in df1_atc_counts.index]\n",
    "    df1_atc_counts.reset_index(inplace=True)\n",
    "    \n",
    "    df2_atc_counts = df2_atc.groupby('level1_description').count().sort_values(by='molregno',ascending=False)[['molregno']]\n",
    "    df2_atc_counts.index = [i.lower().capitalize() for i in df2_atc_counts.index]\n",
    "    df2_atc_counts.reset_index(inplace=True)\n",
    "\n",
    "    comparison = background_atc_counts.merge(df1_atc_counts, how='outer', on='index').merge(df2_atc_counts, how='outer', on='index')\n",
    "    comparison.columns = ['ATC level 1 description', 'Small molecule approved drugs count', f'Drugs in {df1_name} dataset count', f'Drugs in {df2_name} dataset count']\n",
    "    comparison.set_index('ATC level 1 description', drop=True,inplace=True)\n",
    "\n",
    "    comparison['Small molecule approved drugs %'] = comparison['Small molecule approved drugs count'].apply(lambda x: x/sum(comparison['Small molecule approved drugs count'])*100)\n",
    "    comparison[f'{df1_name} drugs %'] = comparison[f'Drugs in {df1_name} dataset count'].apply(lambda x: x/sum(comparison[f'Drugs in {df1_name} dataset count'])*100)\n",
    "    comparison[f'{df2_name} drugs %'] = comparison[f'Drugs in {df2_name} dataset count'].apply(lambda x: x/sum(comparison[f'Drugs in {df2_name} dataset count'])*100)\n",
    "\n",
    "    comparison_r = comparison.sort_values(by='Small molecule approved drugs %')\n",
    "    ind = np.arange(len(comparison_r))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    width = 0.3\n",
    "\n",
    "    ax.barh(ind, comparison_r['Small molecule approved drugs %'], width, label='Approved small molecule drugs (ChEMBL) ({} ATC codes)'.format(background_len), color='darkgrey')\n",
    "    ax.barh(ind + width, comparison_r[f'{df1_name} drugs %'], width, label='{} drugs ({} ATC codes)'.format(df1_name, df1_len), color=df1_color)\n",
    "    ax.barh(ind + width + width, comparison_r[f'{df2_name} drugs %'], width, label='{} drugs ({} ATC codes)'.format(df2_name, df2_len), color=df2_color)\n",
    "    \n",
    "    ax.set(yticks=ind + width, yticklabels=comparison_r.index, ylim=[2*width - 1, len(comparison_r)])\n",
    "    ax.set_xlabel('Percentage of ATC labels')\n",
    "\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    order = [2,1,0]\n",
    "    plt.legend([handles[idx] for idx in order],[labels[idx] for idx in order],loc=(0,1.05))\n",
    "\n",
    "    #ax.set_xticks([i for i in range(1,21,2)])\n",
    "\n",
    "    current_date = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "    plt.savefig(output_loc + '/{}_ATC_bar_combined.png'.format(current_date) , dpi=200, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_target_class_bar_plot_without_safety(df1, df1_name, df1_color, df2, df2_name, df2_color, output_loc, chembl_target_classification):\n",
    "    \"\"\"chembl_target_classification -- df with chembl target classification, must contain columns accession, level_1, level_2\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.rcdefaults()\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    \n",
    "    sns.set_style(\"white\")\n",
    "    sns.set_style('ticks')\n",
    "    plt.rc('axes', labelsize=16)\n",
    "    \n",
    "    def find_integrated(x):\n",
    "        if x['level_2'] == 'Not available':\n",
    "            return x['level_1']\n",
    "        else:\n",
    "            return x['level_2']\n",
    "    \n",
    "    df1_init = pd.DataFrame(chembl_target_classification.loc[chembl_target_classification['accession'].isin(set(list(df1['accession']))),:])\n",
    "    df2_init = pd.DataFrame(chembl_target_classification.loc[chembl_target_classification['accession'].isin(set(list(df2['accession']))),:])\n",
    "    \n",
    "    for df in [df1_init, df2_init]:\n",
    "        df['integrated_level'] = df.apply(find_integrated, axis=1)\n",
    "    \n",
    "    df1_counts = df1_init.groupby(['integrated_level'])['accession'].count().reset_index()\n",
    "    df2_counts = df2_init.groupby(['integrated_level'])['accession'].count().reset_index()\n",
    "\n",
    "    for df, name in zip([df1_counts, df2_counts], [df1_name, df2_name]):\n",
    "        df.columns = ['Target Class', name]\n",
    "        df[name + ' %'] = df[name].apply(lambda x: x/sum(df[name])*100)\n",
    "           \n",
    "    comparison = df1_counts.merge(df2_counts, how='outer')\n",
    "    comparison.fillna(0, inplace=True)\n",
    "    comparison_r = comparison.sort_values(by=df2_name + ' %')\n",
    "    comparison_r.set_index('Target Class', inplace=True)\n",
    "\n",
    "    ind = np.arange(len(comparison_r))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,11))\n",
    "    width = 0.35\n",
    "    ax.barh(ind, comparison_r[df1_name + ' %'], width, label=df1_name, color=df1_color)\n",
    "    ax.barh(ind + width, comparison_r[df2_name + ' %'], width, label=df2_name, color=df2_color)\n",
    "    \n",
    "    ax.set(yticks=ind + width/2, yticklabels=comparison_r.index, ylim=[2*width - 1, len(comparison_r)])\n",
    "    ax.set_xlabel('Unique targets in dataset (%)')\n",
    "\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    order = [1,0]\n",
    "    plt.legend([handles[idx] for idx in order],[labels[idx] for idx in order], loc='lower right')\n",
    "    \n",
    "    current_date = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "    plt.savefig(output_loc + '/{}_target_class_bar_without_safety.png'.format(current_date) , dpi=200, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_target_class_bar_plot(df1, df1_name, df1_color, df2, df2_name, df2_color, safety_targets, df3_name, df3_color, output_loc, chembl_target_classification):\n",
    "    \"\"\"chembl_target_classification -- df with chembl target classification, must contain columns accession, level_1, level_2\n",
    "    safety_targets -- set of known safety targets\n",
    "    df3_name -- str, name of safety target set\"\"\"\n",
    "    \n",
    "    plt.rcdefaults()\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    \n",
    "    sns.set_style(\"white\")\n",
    "    sns.set_style('ticks')\n",
    "    plt.rc('axes', labelsize=16)\n",
    "    \n",
    "    def find_integrated(x):\n",
    "        if x['level_2'] == 'Not available':\n",
    "            return x['level_1']\n",
    "        else:\n",
    "            return x['level_2']\n",
    "    \n",
    "    df1_init = pd.DataFrame(chembl_target_classification.loc[chembl_target_classification['accession'].isin(set(list(df1['accession']))),:])\n",
    "    df2_init = pd.DataFrame(chembl_target_classification.loc[chembl_target_classification['accession'].isin(set(list(df2['accession']))),:])\n",
    "    df3_init = pd.DataFrame(chembl_target_classification.loc[chembl_target_classification['accession'].isin(set(safety_targets)),:])\n",
    "    \n",
    "    for df in [df1_init, df2_init, df3_init]:\n",
    "        df['integrated_level'] = df.apply(find_integrated, axis=1)\n",
    "    \n",
    "    df1_counts = df1_init.groupby(['integrated_level'])['accession'].count().reset_index()\n",
    "    df2_counts = df2_init.groupby(['integrated_level'])['accession'].count().reset_index()\n",
    "    df3_counts = df3_init.groupby(['integrated_level'])['accession'].count().reset_index()\n",
    "\n",
    "    for df, name in zip([df1_counts, df2_counts, df3_counts], [df1_name, df2_name, df3_name]):\n",
    "        df.columns = ['Target Class', name]\n",
    "        df[name + ' %'] = df[name].apply(lambda x: x/sum(df[name])*100)\n",
    "           \n",
    "    comparison = df1_counts.merge(df2_counts, how='outer').merge(df3_counts, how='outer')\n",
    "    comparison.fillna(0, inplace=True)\n",
    "    comparison_r = comparison.sort_values(by=df2_name + ' %')\n",
    "    comparison_r.set_index('Target Class', inplace=True)\n",
    "\n",
    "    ind = np.arange(len(comparison_r))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,11))\n",
    "    width = 0.3\n",
    "    ax.barh(ind, comparison_r[df1_name + ' %'], width, label=df1_name, color=df1_color)\n",
    "    ax.barh(ind + width, comparison_r[df2_name + ' %'], width, label=df2_name, color=df2_color)\n",
    "    ax.barh(ind + width + width, comparison_r[df3_name + ' %'], width, label=df3_name, color=df3_color)\n",
    "    \n",
    "    ax.set(yticks=ind + width, yticklabels=comparison_r.index, ylim=[2*width - 1, len(comparison_r)])\n",
    "    ax.set_xlabel('Unique targets in dataset (%)')\n",
    "\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    order = [2,1,0]\n",
    "    plt.legend([handles[idx] for idx in order],[labels[idx] for idx in order])\n",
    "    \n",
    "    current_date = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "    plt.savefig(output_loc + '/{}_target_class_bar.png'.format(current_date) , dpi=200, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_curve(associations_df, known_merged, sort_by_columns, ascending=False):\n",
    "    \n",
    "    associations_df_upper = associations_df.copy()\n",
    "    associations_df_upper['Adverse Event'] = associations_df_upper['Adverse Event'].apply(lambda x: x.upper())\n",
    "    \n",
    "    known_tuples = set([(x[1]['Accession'],x[1]['HLT'].upper()) for x in known_merged.iterrows()])\n",
    "    \n",
    "    def find_known(row):\n",
    "        if ((row['accession'],row['HLT'])) in known_tuples:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    associations_df_upper['known'] = associations_df_upper.apply(find_known, axis=1)\n",
    "    associations_df_upper_known = associations_df_upper.loc[associations_df_upper['known']==1,:]\n",
    "    \n",
    "    known_retrieved_total_tuples = set([(x[1]['accession'],x[1]['HLT'].upper()) for x in associations_df_upper_known.iterrows()])\n",
    "                \n",
    "    associations_df_sorted = associations_df_upper.sort_values(by=sort_by_columns, ascending=ascending)\n",
    "    \n",
    "    associations_unique = associations_df_sorted.drop_duplicates(subset=['accession', 'Adverse Event'], keep='first')\n",
    "    nr_associations_tested = len(associations_unique)\n",
    "    \n",
    "    frac_considered = []\n",
    "    frac_recalled = []\n",
    "\n",
    "    for threshold in np.linspace(start=0.0, stop=1.0, num=100):\n",
    "        frac_considered.append(threshold)\n",
    "\n",
    "        current_df = associations_unique[:int(threshold*len(associations_unique))]\n",
    "        retrieved_tuples = set([(i,j) for i, j in zip(current_df['accession'], current_df['HLT'])])\n",
    "\n",
    "        nr_found = len(retrieved_tuples & known_retrieved_total_tuples)\n",
    "        frac_recalled.append(nr_found/len(known_retrieved_total_tuples))\n",
    "    \n",
    "    nr_targets = len(set(known_merged['Accession']) & set(associations_df['accession']))\n",
    "    return frac_considered, frac_recalled, nr_targets, nr_associations_tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cumulative_recall(df1, df1_name, df1_color, df2, df2_name, df2_color, known_merged, meddra_hier, sort_by_columns, ascending, output_loc):\n",
    "    plt.rcdefaults()\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.figure(figsize=(8, 6)) \n",
    "    \n",
    "    sns.set_style(\"white\")\n",
    "    plt.rc('axes', labelsize=15)\n",
    "\n",
    "    df1_merged = df1.merge(meddra_hier, left_on='Adverse Event', right_on=' Term')\n",
    "    df2_merged = df2.merge(meddra_hier, left_on='Adverse Event', right_on=' Term')\n",
    "    \n",
    "    df1_considered, df1_recalled, df1_targets_considered, df1_nr_associations_tested = cumulative_curve(df1_merged, sort_by_columns=sort_by_columns, ascending=ascending, known_merged=known_merged)\n",
    "    df2_considered, df2_recalled, df2_targets_considered, df2_nr_associations_tested = cumulative_curve(df2_merged, sort_by_columns=sort_by_columns, ascending=ascending, known_merged=known_merged)\n",
    "    \n",
    "    df1_auc = metrics.auc(df1_considered, df1_recalled)\n",
    "    df2_auc = metrics.auc(df2_considered, df2_recalled)\n",
    "    \n",
    "    x1,y1=[0,1],[0,max(df1_recalled)]\n",
    "    x2,y2=[0,1],[0,max(df2_recalled)]\n",
    "    plt.plot(x1,y1,color='black', alpha=0.5)\n",
    "    plt.plot(x2,y2,color='black', alpha=0.5)\n",
    "    \n",
    "    plt.step(x=df1_considered,y=df1_recalled,alpha=1, where='post',label='{} (AUC = {:.3f})'.format(df1_name, df1_auc), color=df1_color)\n",
    "    plt.step(x=df2_considered,y=df2_recalled,alpha=1, where='post',linestyle=':',label='{} (AUC = {:.3f})'.format(df2_name, df2_auc), color=df2_color)\n",
    "    \n",
    "    max_value = max(df1_recalled + df2_recalled)\n",
    "    plt.axis([0,1,0,max_value+0.05])\n",
    "    plt.legend()\n",
    "    plt.ylabel('Fraction recalled')\n",
    "    plt.xlabel('Fraction of associations considered')\n",
    "    plt.text(1.1,max_value/2,'{}:\\n{} targets overlap with extracted safety targets\\nTotal number of target-AE(HLT) pairs = {}\\nTotal number of associations = {}'.format(df1_name, df1_targets_considered, len(df1_merged[['accession','HLT']].drop_duplicates()), df1_nr_associations_tested), size=12)\n",
    "    \n",
    "    plt.text(1.1,max_value/4,'{}:\\n{} targets overlap with exctracted safety targets\\nTotal number of target-AE(HLT) pairs = {}\\nTotal number of associations = {}'.format(df2_name, df2_targets_considered, len(df2_merged[['accession','HLT']].drop_duplicates()), df2_nr_associations_tested), size=12)\n",
    "    current_date = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "    plt.savefig(output_loc + '/{}_cumulative_recall_hlt_normalised.png'.format(current_date), dpi=200, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_recall_at_pv_cutoff(associations_df, pv_cutoff, known_values, all_targets, all_target_ae_combinations):\n",
    "    associations_df_upper = associations_df.copy()\n",
    "    associations_df_upper['Adverse Event'] = associations_df_upper['Adverse Event'].apply(lambda x: x.upper())\n",
    "    \n",
    "    associations_unique = associations_df_upper.drop_duplicates(subset=['accession', 'Adverse Event'], keep='first')\n",
    "\n",
    "    significant = associations_unique.loc[associations_unique['corrected p-value']<=pv_cutoff,:]\n",
    "\n",
    "    accession2lrs = dict()\n",
    "    for accession in all_targets:\n",
    "        accession2lrs[accession] = dict()\n",
    "    for item in significant.iterrows():\n",
    "        current_accession = item[1]['accession']\n",
    "        current_ae = item[1]['HLT']\n",
    "        current_lr = item[1]['Likelihood Ratio']\n",
    "        accession2lrs[current_accession][current_ae] = current_lr\n",
    "\n",
    "    # Make accession 2 ae vector\n",
    "    ae_values = []\n",
    "    for combination in all_target_ae_combinations:\n",
    "        accession = combination[0]\n",
    "        ae = combination[1]\n",
    "        try:\n",
    "            lr = accession2lrs[accession][ae]\n",
    "            if lr == np.inf:\n",
    "                ae_values.append(100)\n",
    "            else:\n",
    "                ae_values.append(lr)\n",
    "        except KeyError:\n",
    "            ae_values.append(0)\n",
    "            \n",
    "    precision, recall, thresholds = precision_recall_curve(known_values, ae_values)\n",
    "    \n",
    "    return precision, recall, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_recall(associations_df, lr, pv, known_merged, meddra_hier):\n",
    "    \"\"\"Calculate recall at specified LR and q-value thresholds. Restrict to targets and AEs present in dataset\n",
    "    , but do not normalise to 1 based on pairs present.\"\"\"\n",
    "    \n",
    "    associations_df_upper = associations_df.copy()\n",
    "    associations_df_upper['Adverse Event'] = associations_df_upper['Adverse Event'].apply(lambda x: x.upper())\n",
    "    associations_df_sorted = associations_df_upper.sort_values(by=['corrected p-value', 'Likelihood Ratio'], ascending=[True, False])\n",
    "    associations_unique = associations_df_sorted.drop_duplicates(subset=['accession', 'Adverse Event'], keep='first')\n",
    "    \n",
    "    associations_merged = associations_unique.merge(meddra_hier, left_on='Adverse Event', right_on=' Term')\n",
    "    \n",
    "    occurring_targets = set(list(associations_merged['accession']))\n",
    "    occurring_aes = set(list(associations_merged['HLT']))\n",
    "    \n",
    "    # Restrict known associations to the targets and AEs occurring in associations df\n",
    "    known_restricted = known_merged.loc[(known_merged['Accession'].isin(occurring_targets))&(known_merged['HLT'].isin(occurring_aes)),:]\n",
    "    known_tuples = set([(x[1]['Accession'],x[1]['HLT'].upper()) for x in known_restricted.iterrows()])\n",
    "    \n",
    "    associations_cutoff = associations_merged.loc[(associations_merged['Likelihood Ratio']>=lr)&(associations_merged['corrected p-value']<=pv),:]\n",
    "    retrieved_tuples = set([(i,j) for i, j in zip(associations_cutoff['accession'], associations_cutoff['HLT'])])\n",
    "\n",
    "    nr_found = len(retrieved_tuples & known_tuples)\n",
    "    recall = nr_found/len(known_tuples)\n",
    "\n",
    "    info = f\"\"\"Recall of known associations (at HLT level) restricted to those targets and AEs (HLT) that occur at least once in the dataset.\\nAt thresholds LR>={lr} and q-value<={pv}, recall is {recall}, which is {nr_found} out of {len(known_tuples)} known associations (AE-HLT pairs). Number of unique target-HLT associations considered at this threshold: {len(retrieved_tuples)}\\n\"\"\"\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pr_plot_and_txt(all_associations_df, meddra_hier, known_merged, pv_cutoffs, y_lim, x_lim, output_loc):\n",
    "    \n",
    "    plt.rcdefaults()\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.figure(figsize=(8, 6)) \n",
    "    \n",
    "    all_associations_df['Adverse Event'] = all_associations_df['Adverse Event'].apply(lambda x: x.upper())\n",
    "    all_associations_df = all_associations_df.drop_duplicates(subset=['accession', 'Adverse Event'], keep='first')\n",
    "    all_associations_df = all_associations_df.merge(meddra_hier, left_on='Adverse Event', right_on=' Term')\n",
    "    \n",
    "    # Make set of unique AEs to loop over later\n",
    "    all_aes = sorted(list(all_associations_df['HLT'].drop_duplicates()))\n",
    "    all_targets = sorted(list(all_associations_df['accession'].drop_duplicates()))\n",
    "\n",
    "    # Restrict known associations to targets covered in the dataset\n",
    "    current_targets = list(set(all_associations_df['accession']))\n",
    "    current_known_associations = known_merged.loc[(known_merged['Accession'].isin(current_targets))&(known_merged['HLT'].isin(all_aes)),:]\n",
    "    \n",
    "    all_target_ae_combinations = sorted(list(itertools.product(sorted(all_targets), sorted(all_aes))))\n",
    "    \n",
    "    known_tuples = set([(x[1]['Accession'],x[1]['HLT'].upper()) for x in current_known_associations.iterrows()])\n",
    "    known_values = [1 if combination in known_tuples else 0 for combination in all_target_ae_combinations]\n",
    "\n",
    "    colours = iter(['r', 'b', 'darkorange', 'teal'])\n",
    "    \n",
    "    cutoff_dfs = []\n",
    "    \n",
    "    for cutoff in pv_cutoffs:\n",
    "        precision, recall, thresholds = pr_recall_at_pv_cutoff(associations_df=all_associations_df, pv_cutoff=cutoff, known_values=known_values, all_targets=all_targets, all_target_ae_combinations=all_target_ae_combinations)\n",
    "\n",
    "        plt.step(recall, precision, alpha=0.5, color = next(colours), where='post',label='q-value <= {}'.format(cutoff))\n",
    "        \n",
    "        selected_thresholds = pd.DataFrame({'Precision': precision[:-1], 'Recall': recall[:-1], 'LR Threshold': thresholds})\n",
    "        selected_thresholds['q-value'] = cutoff\n",
    "        cutoff_dfs.append(selected_thresholds[['LR Threshold', 'q-value', 'Precision', 'Recall']].drop_duplicates())\n",
    "        \n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, y_lim])\n",
    "    plt.xlim([0.0, x_lim])\n",
    "    \n",
    "    current_date = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "    plt.savefig(output_loc + '/{}_PR_cutoffs.png'.format(current_date), bbox_inches='tight', dpi = 199)\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    \n",
    "    concatenated_cutoff_dfs = pd.concat(cutoff_dfs).sort_values(by='Recall', ascending=False)\n",
    "    concatenated_cutoff_dfs.sort_values(by=['LR Threshold', 'q-value'], ascending=[True, True], inplace=True)\n",
    "    concatenated_cutoff_dfs['LR Threshold'] = concatenated_cutoff_dfs['LR Threshold'].apply(lambda x: '{:.1f}'.format(x))\n",
    "    concatenated_cutoff_dfs['Precision'] = concatenated_cutoff_dfs['Precision'].apply(lambda x: '{:.2f}'.format(x))\n",
    "    concatenated_cutoff_dfs['Recall'] = concatenated_cutoff_dfs['Recall'].apply(lambda x: '{:.2f}'.format(x))\n",
    "    concatenated_cutoff_dfs.drop_duplicates(inplace=True)\n",
    "    \n",
    "    def find_nr_assoc(x):\n",
    "        current_lr = x['LR Threshold']\n",
    "        current_pv = x['q-value']\n",
    "        nr_assoc = len(all_associations_df.loc[(all_associations_df['Likelihood Ratio']>=float(current_lr))&(all_associations_df['corrected p-value']<=float(current_pv)),['accession', 'Adverse Event']].drop_duplicates())\n",
    "        return nr_assoc\n",
    "\n",
    "    concatenated_cutoff_dfs['Number of associations at this threshold'] = concatenated_cutoff_dfs.apply(find_nr_assoc, axis=1)\n",
    "    concatenated_cutoff_dfs.to_csv(output_loc + '/{}_PR_cutoffs.txt'.format(current_date), index=False, sep='\\t')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pv_lr_dist(associations_df, meddra_hier, known_merged, output_dir):\n",
    "    plt.rcdefaults()\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.rcParams[\"patch.force_edgecolor\"] = False\n",
    "    plt.figure(figsize=(8, 6)) \n",
    "    \n",
    "    sns.set_style(\"white\")\n",
    "    \n",
    "    known_tuples = set([(x[1]['Accession'],x[1]['HLT'].upper()) for x in known_merged.iterrows()])\n",
    "    \n",
    "    def find_known(row):\n",
    "        if ((row['accession'],row['HLT'])) in known_tuples:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    associations_df_upper = associations_df.copy()\n",
    "    associations_df_upper['Adverse Event'] = associations_df_upper['Adverse Event'].apply(lambda x: x.upper())\n",
    "    associations_df_sorted = associations_df_upper.sort_values(by=['corrected p-value', 'Likelihood Ratio'], ascending=[True, False])\n",
    "    associations_unique = associations_df_sorted.drop_duplicates(subset=['accession', 'Adverse Event'], keep='first')\n",
    "    \n",
    "    associations_unique = associations_unique.merge(meddra_hier, left_on='Adverse Event', right_on=' Term')\n",
    "    \n",
    "    associations_unique['known'] = associations_unique.apply(find_known, axis=1)\n",
    "    associations_known = associations_unique.loc[associations_unique['known']==1,:]\n",
    "    associations_rest = associations_unique.loc[associations_unique['known']==0,:]\n",
    "    \n",
    "    current_date = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "    # LRs\n",
    "    known_lr = associations_known.loc[associations_known['Likelihood Ratio']<5,'Likelihood Ratio']\n",
    "    rest_lr = associations_rest.loc[associations_rest['Likelihood Ratio']<5,'Likelihood Ratio']\n",
    "    \n",
    "    ax = sns.distplot(known_lr, label='Previously reported associations (n={})'.format(str(len(known_lr))), kde=False, norm_hist=True, color='mediumpurple', hist_kws=dict(edgecolor=\"mediumpurple\", linewidth=0), bins=50)\n",
    "    sns.distplot(rest_lr, label='Other pairs (n={})'.format(str(len(rest_lr))), kde=False, norm_hist=True, color='grey', hist_kws=dict(edgecolor=\"grey\", linewidth=0), bins=50)\n",
    "\n",
    "    ax.set(xlabel='Likelihood Ratio (up to 5)')\n",
    "    ax.set(ylabel='Density')\n",
    "\n",
    "    plt.legend(prop={'size': 12}, loc=1) # bbox_to_anchor=(0.5, -0.15)\n",
    "    ax.tick_params(labelsize=12)\n",
    "    plt.xlim(1,5)\n",
    "\n",
    "    plt.savefig(output_dir + '/{}_LR_dist_3pub-hlt.png'.format(current_date), bbox_inches='tight', dpi = 199)\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    # P-values full range\n",
    "    known_pv = associations_known['corrected p-value']\n",
    "    rest_pv = associations_rest['corrected p-value']\n",
    "\n",
    "    ax = sns.distplot(known_pv, label='Previously reported associations (n={})'.format(str(len(known_pv))), kde=False, norm_hist=True, color='mediumpurple', bins=50, hist_kws=dict(edgecolor=\"mediumpurple\", linewidth=0))\n",
    "    sns.distplot(rest_pv, label='Other pairs (n={})'.format(str(len(rest_pv))), kde=False, norm_hist=True, color='grey', hist_kws=dict(edgecolor=\"grey\", linewidth=0), bins=50)\n",
    "\n",
    "    ax.set(xlabel='q-value')\n",
    "    ax.set(ylabel='Density')\n",
    "\n",
    "    plt.legend(prop={'size': 12}, loc=2) #bbox_to_anchor=(0.5, -0.15)\n",
    "    ax.tick_params(labelsize=12)\n",
    "    plt.xlim(0,1.01)\n",
    "    \n",
    "    \n",
    "    plt.savefig(output_dir + '/{}_pvalue_dist_3pub-hlt-fullt.png'.format(current_date), bbox_inches='tight', dpi = 199)\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    \n",
    "    # P-values low range\n",
    "    known_pv = associations_known.loc[associations_known['corrected p-value']<=0.3,'corrected p-value']\n",
    "    rest_pv = associations_rest.loc[associations_rest['corrected p-value']<=0.3,'corrected p-value']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5,8))\n",
    "    ax = sns.distplot(known_pv, label='Previously reported associations (n={})'.format(str(len(known_pv))), kde=False, norm_hist=True, color='mediumpurple', hist_kws=dict(edgecolor=\"mediumpurple\", linewidth=0), bins=15)\n",
    "    sns.distplot(rest_pv, label='Other pairs (n={})'.format(str(len(rest_pv))), kde=False, norm_hist=True, color='grey', hist_kws=dict(edgecolor=\"grey\", linewidth=0), bins=15)\n",
    "\n",
    "    ax.set(xlabel='q-value')\n",
    "    ax.set(ylabel='Density')\n",
    "\n",
    "    plt.legend(prop={'size': 12}, loc=1) # bbox_to_anchor=(0.5, -0.15)\n",
    "    ax.tick_params(labelsize=12)\n",
    "    plt.xlim(0,0.3)\n",
    "\n",
    "    plt.savefig(output_dir + '/{}_pvalue_dist_3pub-hlt.png'.format(current_date), bbox_inches='tight', dpi = 199)\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_integrated(x):\n",
    "    if x['level_2'] == 'Not available':\n",
    "        return x['level_1']\n",
    "    else:\n",
    "        return x['level_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sign_target_overview_table(df1, df1_name, df2, df2_name, chembl_target_classification, known_merged, output_loc):\n",
    "    known_targets = set(list(known_merged['Accession'].drop_duplicates()))\n",
    "    \n",
    "    all_sign = pd.concat([df1, df2], sort=False)\n",
    "\n",
    "    both_targets = set(list(df1['accession'])) & set(list(df2['accession']))\n",
    "    df1_only_targets = set(list(df1['accession'])) - set(list(df2['accession']))\n",
    "    df2_only_targets = set(list(df2['accession'])) - set(list(df1['accession']))\n",
    "    \n",
    "    both_sign = all_sign.loc[all_sign['accession'].isin(both_targets)]\n",
    "    both_df = both_sign.merge(chembl_target_classification, left_on='accession', right_on='accession')\n",
    "    both_df['dataset'] = 'Both'\n",
    "\n",
    "    df1_only = df1.loc[df1['accession'].isin(df1_only_targets),['accession','pref_name']].drop_duplicates().merge(chembl_target_classification, left_on='accession', right_on='accession')\n",
    "    df1_only['dataset'] = df1_name\n",
    "    df2_only = df2.loc[df2['accession'].isin(df2_only_targets),['accession','pref_name']].drop_duplicates().merge(chembl_target_classification, left_on='accession', right_on='accession')\n",
    "    df2_only['dataset'] = 'SIDER'\n",
    "    \n",
    "    sign_target_overview = pd.concat([df1_only, both_df, df2_only], sort=False)\n",
    "    sign_target_overview['integrated_level'] = sign_target_overview.apply(find_integrated, axis=1)\n",
    "    \n",
    "    sign_target_selected = sign_target_overview[['accession', 'pref_name', 'integrated_level', 'dataset']]\n",
    "    sign_target_selected.columns = ['accession', 'Target name', 'Target Class', 'dataset']\n",
    "    sign_target_selected.drop_duplicates(inplace=True)\n",
    "            \n",
    "    sign_target_selected['Previously reported safety target'] = sign_target_selected['accession'].apply(lambda x: 1 if x in known_targets else 0)\n",
    "    \n",
    "    sorter = ['FAERS', 'Both', 'SIDER']\n",
    "    sorter_dict = dict(zip(sorter,range(len(sorter))))\n",
    "    sign_target_selected['rank'] = sign_target_selected['dataset'].map(sorter_dict)\n",
    "    \n",
    "    sign_target_selected.sort_values(by=['Previously reported safety target', 'rank', 'Target Class'], inplace=True)\n",
    "    sign_target_selected.drop(labels='rank', axis=1, inplace=True)\n",
    "    \n",
    "    current_date = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "    sign_target_selected.to_csv(output_loc + '/{}_sign_target_overview.txt'.format(current_date), index=False, sep='\\t')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actives_per_target(assoc_df1, dataset1_name, df1_color, assoc_df2, dataset2_name, df2_color, output_loc):\n",
    "    plt.rcdefaults()\n",
    "    plt.rc('axes', labelsize=16)\n",
    "    plt.rcParams['xtick.labelsize'] = 16\n",
    "    plt.rcParams['ytick.labelsize'] = 16\n",
    "    plt.rcParams[\"patch.force_edgecolor\"] = False\n",
    "    \n",
    "    active_counts1 = assoc_df1[['accession','nr compounds active']].drop_duplicates()\n",
    "    active_counts1.rename(columns = {'nr compounds active': 'Number of actives per protein'}, inplace=True)\n",
    "\n",
    "    active_counts2 = assoc_df2[['accession','nr compounds active']].drop_duplicates()\n",
    "    active_counts2.rename(columns = {'nr compounds active': 'Number of actives per protein'}, inplace=True)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.hist(active_counts1['Number of actives per protein'].values, bins=30, alpha=0.6, label=dataset1_name + f' ({len(active_counts1)} proteins)', color=df1_color)\n",
    "    plt.hist(active_counts2['Number of actives per protein'].values, bins=30, alpha=0.6, label=dataset2_name + f' ({len(active_counts2)} proteins)', color=df2_color)\n",
    "\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Number of active drugs per protein')\n",
    "    plt.legend(loc=1, fontsize=14)\n",
    "    current_date = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "    plt.savefig(f'{output_loc}/{current_date}_pos_actives_hist.png', dpi=200, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
