{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cases wich multiple targets/bioactivities available, try all combinations of associated targets and find highest overall recall and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import analysis_functions\n",
    "import itertools\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/scratch/ias41/ae_code'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(basedir + '/analysis/data/dirs_info.pkl', 'rb') as f:\n",
    "    dirs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faers_data = dirs['20200110_faers_unbound_margin_pred_005_PRR2']\n",
    "sider_data = dirs['20200110_sider_unbound_margin_pred']\n",
    "combined_destination_dir = 'unbound_margin_pred_faers_vs_sider'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(basedir + '/ae_target_links/output/' + faers_data['dir'] + '/combinations')\n",
    "os.mkdir(basedir + '/ae_target_links/output/' + sider_data['dir'] + '/combinations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faers_pickle = basedir + '/faers_aes/results/20200108_PSM_molregno2aes_PRR2_chi4_faers_min5drugs_all_random_controls.pkl'\n",
    "sider_pickle = basedir + '/sider/results/20191215_molregno2aes_sider_min5drugs.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_info_file = basedir + '/ae_target_links/data/target_names.txt'\n",
    "bioact_data_file = basedir + '/integration_bioact_plasma_conc/results/unbound_median_margin_min5active_added_preds_measured_proteins.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open drug2ae dictionaries\n",
    "with open(faers_pickle, 'rb') as f:\n",
    "    molregno2aes_all = pickle.load(f)\n",
    "    \n",
    "# Find compounds with no significantly associated AEs\n",
    "no_info_molregnos = [molregno for molregno in molregno2aes_all if len(molregno2aes_all[molregno])==0]\n",
    "nr_compounds_without_aes = len(no_info_molregnos)\n",
    "# Restrict to drugs with at least one AE\n",
    "molregno2aes = molregno2aes_all.copy()\n",
    "for molregno in no_info_molregnos:\n",
    "    del(molregno2aes[molregno])\n",
    "assert nr_compounds_without_aes == len(molregno2aes_all) - len(molregno2aes)\n",
    "\n",
    "# Reverse dictionary\n",
    "ae2molregno = {}\n",
    "for molregno in molregno2aes:\n",
    "    for AE in molregno2aes[molregno]:\n",
    "        try:\n",
    "            ae2molregno[AE].add(molregno)\n",
    "        except KeyError:\n",
    "            ae2molregno[AE] = {molregno}\n",
    "\n",
    "with open(sider_pickle, 'rb') as f:\n",
    "    molregno2aes_all_sider = pickle.load(f)\n",
    "    \n",
    "# Reverse dictionary\n",
    "ae2molregno_sider = {}\n",
    "for molregno in molregno2aes_all_sider:\n",
    "    for AE in molregno2aes_all_sider[molregno]:\n",
    "        try:\n",
    "            ae2molregno_sider[AE].add(molregno)\n",
    "        except KeyError:\n",
    "            ae2molregno_sider[AE] = {molregno}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target information\n",
    "target_info = pd.read_csv(target_info_file, sep='\\t')\n",
    "target_info = target_info.loc[target_info['accession_organism']=='Homo sapiens',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open bioactivity data\n",
    "bioact_data = pd.read_csv(bioact_data_file, sep='\\t')\n",
    "\n",
    "# Restrict species\n",
    "#bioact_data = bioact_data.loc[bioact_data['accession'].isin(set(target_info['accession'])),:]\n",
    "\n",
    "# Restrict bioactivity to dataset to those compounds in the current dataset (sider/faers) and at least one AE\n",
    "bioact_df = bioact_data.loc[bioact_data['parent_molregno'].isin(molregno2aes.keys()),:]\n",
    "bioact_df_sider = bioact_data.loc[bioact_data['parent_molregno'].isin(ae2molregno_sider.keys()),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MedDRA hierchy\n",
    "meddra_hier = pd.read_excel(basedir + '/analysis/data/all_faers_and_sider_aes_hier_output.xlsx', skiprows=4)\n",
    "meddra_hier_selection = meddra_hier.loc[meddra_hier['Primary SOC']=='Y',[' Term','HLT','SOC','PT']].drop_duplicates()\n",
    "meddra_hier_selection['HLT'] = meddra_hier_selection['HLT'].apply(lambda x: x.upper())\n",
    "\n",
    "# Previously reported associations\n",
    "# Known associations, merge with known hierarchy HLT\n",
    "known_associations = pd.read_excel(basedir + '/prev_reported_safety_associations/data/safety_meddra_annotated_effects.xls')\n",
    "known_associations['Annotated MedDRA PT'] = known_associations['Annotated MedDRA PT'].apply(lambda x: x.upper())\n",
    "known_meddra_hier = pd.read_excel(basedir + '/prev_reported_safety_associations/data/safety_meddra_annotated_effects_for_hierarchy_output.xlsx', skiprows=4)\n",
    "known_meddra_hier['PT'] = known_meddra_hier['PT'].apply(lambda x: x.upper())\n",
    "known_meddra_hier[' Term'] = known_meddra_hier[' Term'].apply(lambda x: x.upper())\n",
    "known_meddra_hier['HLT'] = known_meddra_hier['HLT'].apply(lambda x: x.upper())\n",
    "known_meddra_hier_selection = known_meddra_hier.loc[known_meddra_hier['Primary SOC']=='Y',['PT','HLT',' Term']].drop_duplicates()\n",
    "known_merged = known_associations.merge(known_meddra_hier_selection, left_on='Annotated MedDRA PT', right_on=' Term')\n",
    "\n",
    "hlt_manual = pd.read_excel(basedir + '/prev_reported_safety_associations/data/safety_meddra_manually_annotated_hlt_effects.xls', index=False)\n",
    "hlt_manual.rename(columns={'Annotated MedDRA HLT': 'HLT'}, inplace=True)\n",
    "hlt_manual['HLT'] = hlt_manual['HLT'].apply(lambda x: x.upper())\n",
    "hlt_manual.drop(columns=['Annotated MedDRA HLT Code'])\n",
    "\n",
    "known_merged = pd.concat([known_merged, hlt_manual], sort=False).reset_index(drop=True)\n",
    "\n",
    "known_tuples = set([(x[1]['Accession'], x[1]['HLT']) for x in known_merged.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_tuples_terms = set([(x[1]['Accession'], x[1][' Term']) for x in known_merged.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load main info from directories\n",
    "faers_pos, faers_sign =  analysis_functions.find_associations(basedir + '/ae_target_links/output/' + faers_data['dir'], min_n=faers_data['min_n'], lr=faers_data['lr'], pv=faers_data['pv'], target_info=target_info)\n",
    "sider_pos, sider_sign = analysis_functions.find_associations(basedir + '/ae_target_links/output/' + sider_data['dir'], min_n=sider_data['min_n'], lr=sider_data['lr'], pv=sider_data['pv'], target_info=target_info)\n",
    "\n",
    "for df in faers_pos, sider_pos, faers_sign, sider_sign:\n",
    "    df['Adverse Event'] = df['Adverse Event'].apply(lambda x: x.upper())\n",
    "    \n",
    "# Calculate Positive predictive value\n",
    "faers_pos['PPV'] = faers_pos.apply(lambda x: analysis_functions.calculate_ppv(x), axis=1)\n",
    "sider_pos['PPV'] = sider_pos.apply(lambda x: analysis_functions.calculate_ppv(x), axis=1)\n",
    "faers_sign['PPV'] = faers_sign.apply(lambda x: analysis_functions.calculate_ppv(x), axis=1)\n",
    "sider_sign['PPV'] = sider_sign.apply(lambda x: analysis_functions.calculate_ppv(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faers_merged = faers_sign.merge(meddra_hier_selection, left_on='Adverse Event', right_on=' Term')\n",
    "sider_merged = sider_sign.merge(meddra_hier_selection, left_on='Adverse Event', right_on=' Term')\n",
    "\n",
    "def find_known(row):\n",
    "    if ((row['accession'],row['Adverse Event'])) in known_tuples_terms:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "faers_merged['known'] = faers_merged.apply(find_known, axis=1)\n",
    "sider_merged['known'] = sider_merged.apply(find_known, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gene symbols for targets\n",
    "accession_string = ' '.join(list(set(faers_merged['accession']) | set(sider_merged['accession'])))\n",
    "url = 'https://www.uniprot.org/uploadlists/'\n",
    "\n",
    "params = {\n",
    "'from': 'UNIPROTKB AC/ID',\n",
    "'to': 'GENENAME',\n",
    "'format': 'tab',\n",
    "'query': f'{accession_string}'\n",
    "}\n",
    "\n",
    "data = urllib.parse.urlencode(params)\n",
    "data = data.encode('utf-8')\n",
    "req = urllib.request.Request(url, data)\n",
    "with urllib.request.urlopen(req) as f:\n",
    "    response = f.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot2gene_id = dict()\n",
    "for item in response.split('\\n')[1:-1]:\n",
    "    uniprot2gene_id[item.split('\\t')[0]] = item.split('\\t')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later use\n",
    "with open(basedir + '/analysis/data/uniprot2gene_id.pkl', 'wb') as f:\n",
    "    pickle.dump(uniprot2gene_id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAERS and SIDER files molregnos are strings, need to turn back into lists to analyse drug overlap\n",
    "def make_into_lists(row):\n",
    "    return [int(float(i)) for i in row.strip('[]').split(', ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faers_merged['ae_vector'] = faers_merged['ae_vector'].apply(lambda x: make_into_lists(x))\n",
    "faers_merged['activity_vector'] = faers_merged['activity_vector'].apply(lambda x: make_into_lists(x))\n",
    "sider_merged['ae_vector'] = sider_merged['ae_vector'].apply(lambda x: make_into_lists(x))\n",
    "sider_merged['activity_vector'] = sider_merged['activity_vector'].apply(lambda x: make_into_lists(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faers_merged['molregnos'] = faers_merged['molregnos'].apply(lambda x: make_into_lists(x))\n",
    "sider_merged['molregnos'] = sider_merged['molregnos'].apply(lambda x: make_into_lists(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faers_merged['active_molregnos'] = faers_merged['active_molregnos'].apply(lambda x: make_into_lists(x))\n",
    "sider_merged['active_molregnos'] = sider_merged['active_molregnos'].apply(lambda x: make_into_lists(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faers_merged['kind'] = 'FAERS'\n",
    "sider_merged['kind'] = 'SIDER'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteratively try combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_all_combinations(AE, assoc_df, dataset, targets=None):\n",
    "    \n",
    "    if dataset=='FAERS':\n",
    "        ae_molregnos = ae2molregno[AE]\n",
    "    elif dataset=='SIDER':\n",
    "        ae_molregnos = ae2molregno_sider[AE]\n",
    "\n",
    "    relevant_df = assoc_df.loc[assoc_df['Adverse Event']==AE,:]\n",
    "    if targets != None:\n",
    "        relevant_df = assoc_df.loc[(assoc_df['Adverse Event']==AE)&(assoc_df['accession'].isin(targets)),:]\n",
    "    \n",
    "    # Find targets associated with AE\n",
    "    target_list = list(relevant_df.loc[relevant_df['Adverse Event']==AE,:].sort_values(by='PPV', ascending=False)['accession'].drop_duplicates())\n",
    "    all_compound_lists = [row[1]['molregnos'] for row in relevant_df.loc[relevant_df['Adverse Event']==AE].iterrows()]\n",
    "    if len(all_compound_lists) == 0:\n",
    "        print(AE + ': no overlap')\n",
    "        return\n",
    "    # Find compounds overlapping between targets\n",
    "    overlapping_compounds = set.intersection(*map(set, all_compound_lists))\n",
    "    if len(overlapping_compounds) == 0:\n",
    "        print(AE + ': no overlap')\n",
    "        return\n",
    "      \n",
    "    nr_compounds_ae_and_measured = len(overlapping_compounds&ae_molregnos)\n",
    "\n",
    "    # Define all combinations\n",
    "    combinations_to_test = set()\n",
    "    len(target_list)\n",
    "    \n",
    "    for i in range(1, len(target_list)+1):\n",
    "        combs = itertools.combinations(target_list, i)\n",
    "        for comb in combs:\n",
    "            combinations_to_test.add(comb)\n",
    "    \n",
    "    # Find performance of that combination\n",
    "    combination_performances = dict()\n",
    "    \n",
    "    for target_set in combinations_to_test:\n",
    "        target_df = bioact_df.loc[bioact_df['accession'].isin(target_set),:]\n",
    "        active_molregnos = set(target_df.loc[target_df['integrated_plasma_activity']==1,'parent_molregno'])\n",
    "        #molregnos = set(target_df['parent_molregno'])\n",
    "        \n",
    "        compounds_found = (active_molregnos & overlapping_compounds).intersection(ae_molregnos)\n",
    "        false_positives = (active_molregnos & overlapping_compounds) - compounds_found\n",
    "        assert len(compounds_found) + len(false_positives) == len(active_molregnos & overlapping_compounds)\n",
    "    \n",
    "        overall_recall = len(compounds_found) / nr_compounds_ae_and_measured\n",
    "        overall_PPV = len(compounds_found) / len(active_molregnos & overlapping_compounds)\n",
    "        \n",
    "        combination_performances[', '.join(list(target_set))] = {'Targets': set(target_set), 'nr_targets': len(target_set), 'Adverse Event': AE, 'Overall_PPV': overall_PPV, 'Overall_recall': overall_recall, 'Compounds screened': len(overlapping_compounds), 'Compounds screened with AE': nr_compounds_ae_and_measured, 'Compounds found': len(compounds_found), 'False positives': len(false_positives)}\n",
    "        \n",
    "    # Find top row based on highest retrieval\n",
    "    performances_df = pd.DataFrame.from_dict(combination_performances, orient='index').sort_values(by=['Overall_recall', 'Overall_PPV'], ascending=[False,False]).reset_index(drop=True)\n",
    "    top_recall = performances_df.head(1)['Overall_recall'][0]\n",
    "    corresponding_PPV = performances_df.head(1)['Overall_PPV'][0]\n",
    "    top_performances = performances_df.loc[(performances_df['Overall_recall']==top_recall)&(performances_df['Overall_PPV']==corresponding_PPV)]\n",
    "\n",
    "    # Find minimum sets with highest performance\n",
    "    min_targets = top_performances['nr_targets'].min()\n",
    "    combinations_of_interest = top_performances.loc[top_performances['nr_targets']==min_targets]\n",
    "    \n",
    "    # Return minimal set with highest performance\n",
    "    return performances_df, combinations_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aes_w_multiple_targets_df = pd.DataFrame(faers_merged.groupby('Adverse Event')[['accession']].count().sort_values(by='accession', ascending=False).query('accession > 1'))\n",
    "aes_w_multiple_targets_df_sider = pd.DataFrame(sider_merged.groupby('Adverse Event')[['accession']].count().sort_values(by='accession', ascending=False).query('accession > 1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sider_cumulative_all = []\n",
    "sider_cumulative_best = []\n",
    "\n",
    "for ae in aes_w_multiple_targets_df_sider.index:\n",
    "    results = try_all_combinations(ae, assoc_df = sider_merged, dataset='SIDER')\n",
    "    sider_cumulative_all.append(results[0])\n",
    "    sider_cumulative_best.append(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faers_cumulative_all = []\n",
    "faers_cumulative_best = []\n",
    "\n",
    "for ae in aes_w_multiple_targets_df.index:\n",
    "    results = try_all_combinations(ae, assoc_df = faers_merged, dataset='FAERS')\n",
    "    faers_cumulative_all.append(results[0])\n",
    "    faers_cumulative_best.append(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sider_all = pd.concat(sider_cumulative_all, ignore_index=True)\n",
    "sider_best = pd.concat(sider_cumulative_best, ignore_index=True)\n",
    "faers_all = pd.concat(faers_cumulative_all, ignore_index=True)\n",
    "faers_best = pd.concat(faers_cumulative_best, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sider_destination_dir = sider_data['dir']\n",
    "sider_all.to_csv(basedir + f'/ae_target_links/output/{sider_destination_dir}/combinations/all_target_combinations.txt', sep='\\t', index=False)\n",
    "sider_best.to_csv(basedir + f'/ae_target_links/output/{sider_destination_dir}/combinations/best_target_combinations.txt', sep='\\t', index=False)\n",
    "\n",
    "faers_destination_dir = faers_data['dir']\n",
    "faers_all.to_csv(basedir + f'/ae_target_links/output/{faers_destination_dir}/combinations/all_target_combinations.txt', sep='\\t', index=False)\n",
    "faers_best.to_csv(basedir + f'/ae_target_links/output/{faers_destination_dir}/combinations/best_target_combinations.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faers_all = pd.read_csv(basedir + f'/ae_target_links/output/{faers_destination_dir}/combinations/all_target_combinations.txt', sep='\\t')\n",
    "\n",
    "faers_best = pd.read_csv(basedir + f'/ae_target_links/output/{faers_destination_dir}/combinations/best_target_combinations.txt', sep='\\t')\n",
    "\n",
    "sider_all = pd.read_csv(basedir + f'/ae_target_links/output/{sider_destination_dir}/combinations/all_target_combinations.txt', sep='\\t')\n",
    "\n",
    "sider_best = pd.read_csv(basedir + f'/ae_target_links/output/{sider_destination_dir}/combinations/best_target_combinations.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faers_best['Target set with highest recall'] = 1\n",
    "faers_all['Target set with highest recall'] = 0\n",
    "sider_best['Target set with highest recall'] = 1\n",
    "sider_all['Target set with highest recall'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faers_best.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'keep='first' only works because I re-imported the file and then the Targets is a string instead of a set... \n",
    "# Combine best and all other combinations FAERS into one file\n",
    "faers_integrated = pd.concat([faers_best, faers_all], sort=False).sort_values(by='Target set with highest recall', ascending=False)\n",
    "faers_integrated2 = faers_integrated.drop_duplicates(subset=['Targets', 'nr_targets', 'Adverse Event', 'Overall_PPV','Overall_recall', 'Compounds screened', 'Compounds screened with AE','Compounds found', 'False positives'], keep='first')\n",
    "\n",
    "faers_integrated2.fillna({'Target set with highest recall': 0}, inplace=True)\n",
    "faers_integrated2.sort_values(by=['Adverse Event','Target set with highest recall'], ascending=[True,False], inplace=True)\n",
    "faers_integrated2['Target set with highest recall'] = faers_integrated2['Target set with highest recall'].astype('int')\n",
    "\n",
    "# Save file\n",
    "faers_integrated2.to_csv(basedir + '/ae_target_links/output/' + faers_data['dir'] + '/combinations/all_combinations_incl_best.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine best and all other combinations SIDER into one file\n",
    "sider_integrated = pd.concat([sider_best, sider_all], sort=False).sort_values(by='Target set with highest recall')\n",
    "sider_integrated2 = sider_integrated.drop_duplicates(subset=['Targets', 'nr_targets', 'Adverse Event', 'Overall_PPV','Overall_recall', 'Compounds screened', 'Compounds screened with AE','Compounds found', 'False positives'], keep='first')\n",
    "sider_integrated2.fillna({'Target set with highest recall': 0}, inplace=True)\n",
    "sider_integrated2.sort_values(by=['Adverse Event','Target set with highest recall'], ascending=[True,False], inplace=True)\n",
    "sider_integrated2['Target set with highest recall'] = sider_integrated2['Target set with highest recall'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file\n",
    "sider_integrated2.to_csv(basedir + '/ae_target_links/output/' + sider_data['dir'] + '/combinations/all_combinations_incl_best.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save 10 best combinations across datasets\n",
    "\n",
    "sider_integrated3 = sider_best.copy()\n",
    "faers_integrated3 = faers_best.copy()\n",
    "sider_integrated3['Dataset'] = 'S'\n",
    "faers_integrated3['Dataset'] = 'F'\n",
    "\n",
    "both_datasets = pd.concat([faers_integrated3, sider_integrated3], sort=False)\n",
    "\n",
    "my_selection = both_datasets.loc[both_datasets['nr_targets']>1].sort_values(by=['Overall_recall','Overall_PPV'], ascending=False).drop_duplicates(subset=['Adverse Event'], keep='first')[:10]\n",
    "my_selection['Target set'] = my_selection['Targets'].apply(lambda x: ', '.join([uniprot2gene_id[i] for i in eval(x)]))\n",
    "my_selection.rename(columns={'Overall_PPV': 'Overall PPV', 'Overall_recall': 'Overall recall'}, inplace=True)\n",
    "my_selection['Adverse Event'] = my_selection['Adverse Event'].apply(lambda x: x.lower().capitalize())\n",
    "my_selection['Overall recall'] = my_selection['Overall recall'].apply(lambda x: '{:.2f}'.format(float(x)))\n",
    "my_selection['Overall PPV'] = my_selection['Overall PPV'].apply(lambda x: '{:.2f}'.format(float(x)))\n",
    "\n",
    "my_selection[['Target set', 'Adverse Event', 'Overall PPV',\n",
    "       'Overall recall','Dataset']].to_csv(basedir + f'/analysis/results/{combined_destination_dir}/top10_recall_combinations.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_combs_df(ae, best_df, all_df, legend_loc='best'):\n",
    "    cumulative_performances = list()\n",
    "    \n",
    "    best_combination = list(best_df.loc[best_df['Adverse Event']==ae,'Targets'])[0]\n",
    "    \n",
    "    combinations_to_test = set()\n",
    "    for i in range(1, len(best_combination)):\n",
    "        combs = itertools.combinations(best_combination, i)\n",
    "        for comb in combs:\n",
    "            combinations_to_test.add(comb)\n",
    "    \n",
    "    for comb in combinations_to_test:\n",
    "        comb_df = all_df.loc[(all_df['Adverse Event']==ae)&(all_df['Targets']==set(comb))]\n",
    "        cumulative_performances.append(comb_df)\n",
    "    \n",
    "    cumulative_performances.append(best_df.loc[best_df['Adverse Event']==ae])\n",
    "    cumulative_performances_df = pd.concat(cumulative_performances, ignore_index=True).sort_values(by=['Overall_recall', 'Overall_PPV']).reset_index(drop=True).reset_index(drop=False)\n",
    "    \n",
    "    mystyles = iter(['solid', 'dashed', 'dotted', (0, (3, 1, 1, 1)), (0, (3, 5, 1, 5)), (0, (1, 10))])\n",
    "    plt.rcdefaults()\n",
    "    plt.rcParams.update({'font.size': 13})\n",
    "    plt.rc('axes', labelsize=16)\n",
    "    \n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    ax = fig.add_subplot(111)\n",
    "    all_texts = []\n",
    "        \n",
    "    my_df = cumulative_performances_df.copy()\n",
    "    if len(my_df) ==1:\n",
    "        return\n",
    "    \n",
    "    def find_superscript_symbol(accession, ae):\n",
    "        gene_id = uniprot2gene_id[accession]\n",
    "        \n",
    "        ae_hlt = term2hlt[ae]\n",
    "        superscript1 = ''\n",
    "        superscript2 = ''\n",
    "        if (accession, ae_hlt) in known_tuples:\n",
    "            superscript1 = r'$^\\blacktriangledown$'\n",
    "        if accession in safety_targets:\n",
    "            superscript2 = r'$^\\triangledown$'\n",
    "        return gene_id + superscript1 + superscript2 \n",
    "            \n",
    "    my_df['gene_ids'] = my_df['Targets'].apply(lambda x: ', '.join([find_superscript_symbol(y,ae) for y in x]))\n",
    "    \n",
    "    compounds_screened_with_ae = list(my_df['Compounds screened with AE'].drop_duplicates())[0]\n",
    "    compounds_screened = list(my_df['Compounds screened'].drop_duplicates())[0]\n",
    "\n",
    "    plt.plot(my_df['index'], my_df['Overall_recall'], '-o', markersize=3,linestyle=next(mystyles), color='black', label=f'Recall (out of n = {compounds_screened_with_ae})')\n",
    "    plt.plot(my_df['index'], my_df['Overall_PPV'], '-o', markersize=3,linestyle=next(mystyles), color='black', label='Positive Predictive Value')\n",
    "\n",
    "    gene_ids = list(my_df['gene_ids'])#[uniprot2gene_id[accession] for accession in list(my_df['Targets'])]\n",
    "\n",
    "    #colors = ['blue' if x==1 else 'black' if y==1 else 'red' for x,y in zip(list(my_df['known association']), list(my_df['known target']))]\n",
    "#     texts = [plt.text(x,y,z, ha='center', va='center', size=12) for x,y,z in zip(my_df['index'], my_df['Overall_recall'],gene_ids)]\n",
    "#     adjust_text(texts, arrowprops=dict(arrowstyle='-', color='darkgrey'))\n",
    "\n",
    "#     for text in texts:\n",
    "#         all_texts.append(text)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    if legend_loc ==None:\n",
    "        plt.legend(bbox_to_anchor=(1,1))\n",
    "    else:\n",
    "        plt.legend(loc=legend_loc)\n",
    "    ax.set_ylabel('Performance')\n",
    "    ax.set_xlabel(f'Target contributions\\nn = {compounds_screened} compounds overlapping')\n",
    "    \n",
    "    ax.set_xticks(list(my_df['index']))\n",
    "    ax.set_xticklabels(gene_ids, rotation=45,horizontalalignment='right')\n",
    "\n",
    "    plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "    return plt.gcf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term to HLT dict\n",
    "term2hlt = dict()\n",
    "for row in meddra_hier_selection.iterrows():\n",
    "    term2hlt[row[1][' Term']] = row[1]['HLT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_targets = set(known_merged['Accession'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sider_best['Targets'] = sider_best['Targets'].apply(lambda x: eval(x))\n",
    "faers_best['Targets'] = faers_best['Targets'].apply(lambda x: eval(x))\n",
    "sider_all['Targets'] = sider_all['Targets'].apply(lambda x: eval(x))\n",
    "faers_all['Targets'] = faers_all['Targets'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ae in list(sider_best['Adverse Event'].drop_duplicates()):\n",
    "    myplot = make_combs_df(ae, sider_best, sider_all)\n",
    "    ae_formatted = ae.replace(' ', '_').replace('/', '-')\n",
    "    if myplot:\n",
    "        myplot.savefig(basedir + f'/ae_target_links/output/{sider_destination_dir}/combinations/{ae_formatted}.jpg', bbox_inches='tight')\n",
    "        plt.close()\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ae in list(faers_best['Adverse Event'].drop_duplicates()):\n",
    "    myplot = make_combs_df(ae, faers_best, faers_all)\n",
    "    ae_formatted = ae.replace(' ', '_').replace('/', '-')\n",
    "    if myplot:\n",
    "        myplot.savefig(basedir + f'/ae_target_links/output/{faers_destination_dir}/combinations/{ae_formatted}.jpg', bbox_inches='tight')\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
